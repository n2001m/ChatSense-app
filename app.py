{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdb1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import tempfile\n",
    "import wave\n",
    "\n",
    "# Load SVM model and scaler\n",
    "loaded_svm_model = joblib.load(\"svm_model_3sec_allF.pkl\")\n",
    "loaded_scaler = joblib.load(\"scaler_3sec_allF.pkl\")\n",
    "\n",
    "# Function to preprocess audio\n",
    "def preprocess_audio(data, sr=16000):\n",
    "    trimmed_data, _ = librosa.effects.trim(data)\n",
    "    return trimmed_data\n",
    "\n",
    "# Function to segment audio\n",
    "def segment_audio(audio, sr=16000, segment_size=3, overlap=0.5):\n",
    "    segment_size_samples = int(segment_size * sr)\n",
    "    hop_length = int(segment_size_samples * (1 - overlap))\n",
    "    \n",
    "    if len(audio) <= segment_size_samples:\n",
    "        return [audio]\n",
    "    \n",
    "    segments = [audio[i:i + segment_size_samples] \n",
    "                for i in range(0, len(audio) - segment_size_samples + 1, hop_length)]\n",
    "    return segments\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(audio, sr=16000):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    energy = np.mean(librosa.feature.rms(y=audio))\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
    "    return np.concatenate([mfcc_mean, [energy, zcr]])\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(segments):\n",
    "    predicted_labels = []\n",
    "    for segment in segments:\n",
    "        features = extract_features(segment)\n",
    "        standardized_features = loaded_scaler.transform(features.reshape(1, -1))\n",
    "        predicted_label = loaded_svm_model.predict(standardized_features)\n",
    "        predicted_labels.append(predicted_label[0])\n",
    "    return predicted_labels\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Real-Time Audio Emotion Recognition\")\n",
    "st.write(\"Record an audio clip and get emotion predictions for each segment.\")\n",
    "\n",
    "# Record Audio\n",
    "duration = st.slider(\"Select Recording Duration (seconds)\", 1, 10, 3)\n",
    "if st.button(\"Record Audio\"):\n",
    "    st.write(\"Recording...\")\n",
    "    fs = 16000\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    st.write(\"Recording complete!\")\n",
    "    \n",
    "    # Process and Predict\n",
    "    audio_data = recording.flatten()\n",
    "    preprocessed_audio = preprocess_audio(audio_data, sr=fs)\n",
    "    segments = segment_audio(preprocessed_audio, sr=fs)\n",
    "    predicted_labels = predict_emotion(segments)\n",
    "    \n",
    "    # Display results\n",
    "    st.write(\"Predicted Emotions for each segment:\")\n",
    "    st.table({\"Segment\": list(range(1, len(predicted_labels)+1)), \"Emotion\": predicted_labels})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
